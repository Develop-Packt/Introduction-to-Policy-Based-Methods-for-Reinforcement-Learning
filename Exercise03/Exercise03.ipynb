{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch as T\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory_actions = []\n",
    "        self.memory_states = []\n",
    "        self.memory_log_probs = []\n",
    "        self.memory_rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.memory_actions[:]\n",
    "        del self.memory_states[:]\n",
    "        del self.memory_log_probs[:]\n",
    "        del self.memory_rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(T.nn.Module):\n",
    "    def __init__(self, state_dimension, action_dimension, nb_latent_variables):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.action_layer = T.nn.Sequential(\n",
    "            T.nn.Linear(state_dimension, nb_latent_variables),\n",
    "            T.nn.Tanh(),\n",
    "            T.nn.Linear(nb_latent_variables, nb_latent_variables),\n",
    "            T.nn.Tanh(),\n",
    "            T.nn.Linear(nb_latent_variables, action_dimension),\n",
    "            T.nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.value_layer = T.nn.Sequential(\n",
    "            T.nn.Linear(state_dimension, nb_latent_variables),\n",
    "            T.nn.Tanh(),\n",
    "            T.nn.Linear(nb_latent_variables, nb_latent_variables),\n",
    "            T.nn.Tanh(),\n",
    "            T.nn.Linear(nb_latent_variables, 1)\n",
    "        )\n",
    "\n",
    "    def act(self, state, memory):\n",
    "        state = T.from_numpy(state).float().to(device) \n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = T.distributions.Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        memory.memory_states.append(state)\n",
    "        memory.memory_actions.append(action)\n",
    "        memory.memory_log_probs.append(dist.log_prob(action))\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_probs = self.action_layer(state)\n",
    "        dist = T.distributions.Categorical(action_probs)\n",
    "\n",
    "        action_log_probs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.value_layer(state)\n",
    "\n",
    "        return action_log_probs, T.squeeze(state_value), dist_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, state_dimension, action_dimension, nb_latent_variables,\n",
    "        lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = ActorCritic(\n",
    "            state_dimension,\n",
    "            action_dimension,\n",
    "            nb_latent_variables).to(device)\n",
    "        self.optimizer = T.optim.Adam(\n",
    "            self.policy.parameters(), lr=lr, betas=betas)\n",
    "        self.policy_old = ActorCritic(\n",
    "            state_dimension,\n",
    "            action_dimension,\n",
    "            nb_latent_variables).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = T.nn.MSELoss()\n",
    "    \n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in \\\n",
    "            zip(reversed(memory.memory_rewards), reversed(memory.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalize\n",
    "        rewards = T.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        old_states = T.stack(memory.memory_states).to(device).detach()\n",
    "        old_actions = T.stack(memory.memory_actions).to(device).detach()\n",
    "        old_log_probs = T.stack(memory.memory_log_probs).to(device).detach()\n",
    "        \n",
    "        # Policy Optimization\n",
    "        for _ in range(self.K_epochs):\n",
    "            log_probs, state_values, dist_entropy = self.policy.evaluate(\n",
    "                old_states, old_actions)\n",
    "            \n",
    "            # Finding ratio: pi_theta / pi_theta__old\n",
    "            ratios = T.exp(log_probs - old_log_probs.detach())\n",
    "            \n",
    "            # Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = T.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -T.min(surr1, surr2) + \\\n",
    "                0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # New weights to old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, reward: -8\n",
      "Episode 20, reward: -182\n",
      "Episode 40, reward: -154\n",
      "Episode 60, reward: -175\n",
      "Episode 80, reward: -136\n",
      "Episode 100, reward: -178\n",
      "Episode 120, reward: -128\n",
      "Episode 140, reward: -137\n",
      "Episode 160, reward: -140\n",
      "Episode 180, reward: -150\n",
      "Episode 200, reward: -119\n",
      "Episode 220, reward: -105\n",
      "Episode 240, reward: -114\n",
      "Episode 260, reward: -94\n",
      "Episode 280, reward: -100\n",
      "Episode 300, reward: -60\n",
      "Episode 320, reward: -60\n",
      "Episode 340, reward: -53\n",
      "Episode 360, reward: -16\n",
      "Episode 380, reward: 30\n",
      "Episode 400, reward: 43\n",
      "Episode 420, reward: 59\n",
      "Episode 440, reward: 33\n",
      "Episode 460, reward: 48\n",
      "Episode 480, reward: 58\n",
      "Episode 500, reward: 68\n",
      "Episode 520, reward: 89\n",
      "Episode 540, reward: 111\n",
      "Episode 560, reward: 69\n",
      "Episode 580, reward: 101\n",
      "Episode 600, reward: 86\n",
      "Episode 620, reward: 117\n",
      "Episode 640, reward: 120\n",
      "Episode 660, reward: 62\n",
      "Episode 680, reward: 117\n",
      "Episode 700, reward: 119\n",
      "Episode 720, reward: 105\n",
      "Episode 740, reward: 106\n",
      "Episode 760, reward: 109\n",
      "Episode 780, reward: 121\n",
      "Episode 800, reward: 119\n",
      "Episode 820, reward: 116\n",
      "Episode 840, reward: 137\n",
      "Episode 860, reward: 129\n",
      "Episode 880, reward: 132\n",
      "Episode 900, reward: 152\n",
      "Episode 920, reward: 129\n",
      "Episode 940, reward: 112\n",
      "Episode 960, reward: 116\n",
      "Episode 980, reward: 144\n",
      "Episode 1000, reward: 143\n",
      "Episode 1020, reward: 145\n",
      "Episode 1040, reward: 133\n",
      "Episode 1060, reward: 149\n",
      "Episode 1080, reward: 144\n",
      "Episode 1100, reward: 148\n",
      "Episode 1120, reward: 143\n",
      "Episode 1140, reward: 105\n",
      "Episode 1160, reward: 163\n",
      "Episode 1180, reward: 159\n",
      "Episode 1200, reward: 162\n",
      "Episode 1220, reward: 124\n",
      "Episode 1240, reward: 142\n",
      "Episode 1260, reward: 140\n",
      "Episode 1280, reward: 148\n",
      "Episode 1300, reward: 166\n",
      "Episode 1320, reward: 154\n",
      "Episode 1340, reward: 139\n",
      "Episode 1360, reward: 158\n",
      "Episode 1380, reward: 149\n",
      "Episode 1400, reward: 159\n",
      "Episode 1420, reward: 155\n",
      "Episode 1440, reward: 159\n",
      "Episode 1460, reward: 157\n",
      "Episode 1480, reward: 169\n",
      "Episode 1500, reward: 141\n",
      "Episode 1520, reward: 135\n",
      "Episode 1540, reward: 139\n",
      "Episode 1560, reward: 140\n",
      "Episode 1580, reward: 155\n",
      "Episode 1600, reward: 136\n",
      "Episode 1620, reward: 108\n",
      "Episode 1640, reward: 141\n",
      "Episode 1660, reward: 137\n",
      "Episode 1680, reward: 103\n",
      "Episode 1700, reward: 146\n",
      "Episode 1720, reward: 149\n",
      "Episode 1740, reward: 115\n",
      "Episode 1760, reward: 118\n",
      "Episode 1780, reward: 102\n",
      "Episode 1800, reward: 87\n",
      "Episode 1820, reward: 95\n",
      "Episode 1840, reward: 114\n",
      "Episode 1860, reward: 83\n",
      "Episode 1880, reward: 113\n",
      "Episode 1900, reward: 102\n",
      "Episode 1920, reward: 99\n",
      "Episode 1940, reward: 142\n",
      "Episode 1960, reward: 116\n",
      "Episode 1980, reward: 105\n",
      "Episode 2000, reward: 86\n",
      "Episode 2020, reward: 120\n",
      "Episode 2040, reward: 109\n",
      "Episode 2060, reward: 38\n",
      "Episode 2080, reward: 84\n",
      "Episode 2100, reward: 99\n",
      "Episode 2120, reward: 84\n",
      "Episode 2140, reward: 85\n",
      "Episode 2160, reward: 118\n",
      "Episode 2180, reward: 158\n",
      "Episode 2200, reward: 123\n",
      "Episode 2220, reward: 149\n",
      "Episode 2240, reward: 154\n",
      "Episode 2260, reward: 125\n",
      "Episode 2280, reward: 133\n",
      "Episode 2300, reward: 86\n",
      "Episode 2320, reward: 116\n",
      "Episode 2340, reward: 116\n",
      "Episode 2360, reward: 66\n",
      "Episode 2380, reward: 128\n",
      "Episode 2400, reward: 148\n",
      "Episode 2420, reward: 116\n",
      "Episode 2440, reward: 82\n",
      "Episode 2460, reward: 84\n",
      "Episode 2480, reward: 75\n",
      "Episode 2500, reward: 85\n",
      "Episode 2520, reward: 126\n",
      "Episode 2540, reward: 103\n",
      "Episode 2560, reward: 66\n",
      "Episode 2580, reward: 93\n",
      "Episode 2600, reward: 60\n",
      "Episode 2620, reward: 52\n",
      "Episode 2640, reward: 98\n",
      "Episode 2660, reward: 123\n",
      "Episode 2680, reward: 121\n",
      "Episode 2700, reward: 75\n",
      "Episode 2720, reward: 131\n",
      "Episode 2740, reward: 102\n",
      "Episode 2760, reward: 111\n",
      "Episode 2780, reward: 125\n",
      "Episode 2800, reward: 120\n",
      "Episode 2820, reward: 118\n",
      "Episode 2840, reward: 116\n",
      "Episode 2860, reward: 111\n",
      "Episode 2880, reward: 119\n",
      "Episode 2900, reward: 107\n",
      "Episode 2920, reward: 103\n",
      "Episode 2940, reward: 92\n",
      "Episode 2960, reward: 121\n",
      "Episode 2980, reward: 126\n",
      "Episode 3000, reward: 139\n",
      "Episode 3020, reward: 145\n",
      "Episode 3040, reward: 102\n",
      "Episode 3060, reward: 126\n",
      "Episode 3080, reward: 145\n",
      "Episode 3100, reward: 146\n",
      "Episode 3120, reward: 116\n",
      "Episode 3140, reward: 110\n",
      "Episode 3160, reward: 146\n",
      "Episode 3180, reward: 145\n",
      "Episode 3200, reward: 143\n",
      "Episode 3220, reward: 82\n",
      "Episode 3240, reward: 122\n",
      "Episode 3260, reward: 123\n",
      "Episode 3280, reward: 125\n",
      "Episode 3300, reward: 129\n",
      "Episode 3320, reward: 124\n",
      "Episode 3340, reward: 114\n",
      "Episode 3360, reward: 146\n",
      "Episode 3380, reward: 163\n",
      "Episode 3400, reward: 155\n",
      "Episode 3420, reward: 140\n",
      "Episode 3440, reward: 151\n",
      "Episode 3460, reward: 161\n",
      "Episode 3480, reward: 116\n",
      "Episode 3500, reward: 125\n",
      "Episode 3520, reward: 142\n",
      "Episode 3540, reward: 112\n",
      "Episode 3560, reward: 140\n",
      "Episode 3580, reward: 153\n",
      "Episode 3600, reward: 146\n",
      "Episode 3620, reward: 167\n",
      "Episode 3640, reward: 100\n",
      "Episode 3660, reward: 129\n",
      "Episode 3680, reward: 136\n",
      "Episode 3700, reward: 148\n",
      "Episode 3720, reward: 164\n",
      "Episode 3740, reward: 144\n",
      "Episode 3760, reward: 156\n",
      "Episode 3780, reward: 132\n",
      "Episode 3800, reward: 146\n",
      "Episode 3820, reward: 133\n",
      "Episode 3840, reward: 144\n",
      "Episode 3860, reward: 154\n",
      "Episode 3880, reward: 146\n",
      "Episode 3900, reward: 138\n",
      "Episode 3920, reward: 145\n",
      "Episode 3940, reward: 145\n",
      "Episode 3960, reward: 118\n",
      "Episode 3980, reward: 128\n",
      "Episode 4000, reward: 105\n",
      "Episode 4020, reward: 149\n",
      "Episode 4040, reward: 130\n",
      "Episode 4060, reward: 91\n",
      "Episode 4080, reward: 133\n",
      "Episode 4100, reward: 90\n",
      "Episode 4120, reward: 127\n",
      "Episode 4140, reward: 130\n",
      "Episode 4160, reward: 115\n",
      "Episode 4180, reward: 106\n",
      "Episode 4200, reward: 126\n",
      "Episode 4220, reward: 140\n",
      "Episode 4240, reward: 120\n",
      "Episode 4260, reward: 99\n",
      "Episode 4280, reward: 118\n",
      "Episode 4300, reward: 100\n",
      "Episode 4320, reward: 114\n",
      "Episode 4340, reward: 123\n",
      "Episode 4360, reward: 129\n",
      "Episode 4380, reward: 104\n",
      "Episode 4400, reward: 115\n",
      "Episode 4420, reward: 120\n",
      "Episode 4440, reward: 132\n",
      "Episode 4460, reward: 133\n",
      "Episode 4480, reward: 145\n",
      "Episode 4500, reward: 110\n",
      "Episode 4520, reward: 140\n",
      "Episode 4540, reward: 127\n",
      "Episode 4560, reward: 145\n",
      "Episode 4580, reward: 107\n",
      "Episode 4600, reward: 143\n",
      "Episode 4620, reward: 128\n",
      "Episode 4640, reward: 101\n",
      "Episode 4660, reward: 112\n",
      "Episode 4680, reward: 150\n",
      "Episode 4700, reward: 118\n",
      "Episode 4720, reward: 135\n",
      "Episode 4740, reward: 93\n",
      "Episode 4760, reward: 89\n",
      "Episode 4780, reward: 132\n",
      "Episode 4800, reward: 138\n",
      "Episode 4820, reward: 146\n",
      "Episode 4840, reward: 126\n",
      "Episode 4860, reward: 123\n",
      "Episode 4880, reward: 110\n",
      "Episode 4900, reward: 133\n",
      "Episode 4920, reward: 115\n",
      "Episode 4940, reward: 120\n",
      "Episode 4960, reward: 109\n",
      "Episode 4980, reward: 113\n",
      "Episode 5000, reward: 160\n",
      "Episode 5020, reward: 125\n",
      "Episode 5040, reward: 161\n",
      "Episode 5060, reward: 158\n",
      "Episode 5080, reward: 164\n",
      "Episode 5100, reward: 135\n",
      "Episode 5120, reward: 133\n",
      "Episode 5140, reward: 131\n",
      "Episode 5160, reward: 147\n",
      "Episode 5180, reward: 130\n",
      "Episode 5200, reward: 128\n",
      "Episode 5220, reward: 135\n",
      "Episode 5240, reward: 115\n",
      "Episode 5260, reward: 126\n",
      "Episode 5280, reward: 115\n",
      "Episode 5300, reward: 149\n",
      "Episode 5320, reward: 149\n",
      "Episode 5340, reward: 143\n",
      "Episode 5360, reward: 166\n",
      "Episode 5380, reward: 117\n",
      "Episode 5400, reward: 113\n",
      "Episode 5420, reward: 168\n",
      "Episode 5440, reward: 143\n",
      "Episode 5460, reward: 133\n",
      "Episode 5480, reward: 96\n",
      "Episode 5500, reward: 124\n",
      "Episode 5520, reward: 77\n",
      "Episode 5540, reward: 96\n",
      "Episode 5560, reward: 96\n",
      "Episode 5580, reward: 122\n",
      "Episode 5600, reward: 131\n",
      "Episode 5620, reward: 105\n",
      "Episode 5640, reward: 145\n",
      "Episode 5660, reward: 145\n",
      "Episode 5680, reward: 107\n",
      "Episode 5700, reward: 129\n",
      "Episode 5720, reward: 163\n",
      "Episode 5740, reward: 131\n",
      "Episode 5760, reward: 84\n",
      "Episode 5780, reward: 134\n",
      "Episode 5800, reward: 138\n",
      "Episode 5820, reward: 129\n",
      "Episode 5840, reward: 125\n",
      "Episode 5860, reward: 129\n",
      "Episode 5880, reward: 141\n",
      "Episode 5900, reward: 164\n",
      "Episode 5920, reward: 133\n",
      "Episode 5940, reward: 132\n",
      "Episode 5960, reward: 154\n",
      "Episode 5980, reward: 158\n",
      "Episode 6000, reward: 156\n",
      "Episode 6020, reward: 168\n",
      "Episode 6040, reward: 168\n",
      "Episode 6060, reward: 151\n",
      "Episode 6080, reward: 166\n",
      "Episode 6100, reward: 148\n",
      "Episode 6120, reward: 144\n",
      "Episode 6140, reward: 158\n",
      "Episode 6160, reward: 102\n",
      "Episode 6180, reward: 146\n",
      "Episode 6200, reward: 131\n",
      "Episode 6220, reward: 154\n",
      "Episode 6240, reward: 197\n",
      "Episode 6260, reward: 163\n",
      "Episode 6280, reward: 181\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "render = False\n",
    "solved_reward = 230\n",
    "logging_interval = 20\n",
    "update_timestep = 2000\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "memory = ReplayBuffer()\n",
    "agent = Agent(\n",
    "    state_dimension=env.observation_space.shape[0],\n",
    "    action_dimension=4,\n",
    "    nb_latent_variables=64,\n",
    "    lr=0.002,\n",
    "    betas=(0.9, 0.999),\n",
    "    gamma=0.99,\n",
    "    K_epochs=4,\n",
    "    eps_clip=0.2)\n",
    "\n",
    "current_reward = 0\n",
    "avg_length = 0\n",
    "timestep = 0\n",
    "\n",
    "for i_ep in range(50000):\n",
    "    state = env.reset()\n",
    "    for t in range(300):\n",
    "        timestep += 1\n",
    "        \n",
    "        action = agent.policy_old.act(state, memory)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        memory.memory_rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "        \n",
    "        if timestep % update_timestep == 0:\n",
    "            agent.update(memory)\n",
    "            memory.clear_memory()\n",
    "            timestep = 0\n",
    "\n",
    "        current_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    avg_length += t\n",
    "    \n",
    "    if current_reward > (logging_interval*solved_reward):\n",
    "        T.save(agent.policy.state_dict(), \"PPO_LunarLander-v2.pth\")\n",
    "        break\n",
    "    \n",
    "    if i_ep % logging_interval == 0:\n",
    "        avg_length = int(avg_length/logging_interval)\n",
    "        current_reward = int((current_reward/logging_interval))\n",
    "\n",
    "        print(\"Episode {}, reward: {}\".format(i_ep, current_reward))\n",
    "        current_reward = 0\n",
    "        avg_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
